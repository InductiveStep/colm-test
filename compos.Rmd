---
title: Was turnout associated with the composition of Labour, Tory, and Reform votes in the 2024 UK general election?
author: "Andi Fugard (Mastodon: @andi@sciences.social)"
date: "17 July 2024"
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: none
  word_document: default
---


This is an excuse to try out the {compos} package, by David Firth and Fiona Sammut. The data used below was collated and published by [Democracy Club](https://candidates.democracyclub.org.uk/data/?election_id=parl.2024-07-04&field_group=results) under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).

The House of Commons Library is due to publish its own voting dataset [here](https://commonslibrary.parliament.uk/research-briefings/cbp-10009/) some time week of 15 July.

Load up some packages ({compos} isn't on CRAN yet):

```{r}
#devtools::install_github("DavidFirth/compos")
library(conflicted)
library(tidyverse)
library(compos)
library(ggmice)
library(mice)
library(table1)
```


## Setup the data

```{r}
#ge_dat <- read_csv("uk_general_election_2024.csv")
ge_dat <- read_csv("uk_general_election_2024_updated_2024-07-17.csv")
```

Filter so we only have Labour, Tory, and Reform.

```{r}
parties <- c("Labour and Co-operative Party",
             "Labour Party",
             "Conservative and Unionist Party",
             "Reform UK")
```


And pivot wider, with all the data we want:

```{r}
simp_dat <- ge_dat |>
  dplyr::filter(party_name %in% parties) |>
  mutate(
    part = case_when(
      party_name == "Labour and Co-operative Party" ~ "Lab",
      party_name == "Labour Party" ~ "Lab",
      party_name == "Conservative and Unionist Party" ~ "Con",
      party_name == "Reform UK" ~ "Ref",     
      .default = NA_character_
    )
  ) |>
  select(post_label, part, votes_cast, turnout_percentage) |>
  pivot_wider(names_from = "part", values_from = "votes_cast")
simp_dat
```


```{r}
table1(~., data=simp_dat |> dplyr::select(-post_label))
```




```{r}
plot_pattern(simp_dat, rotate = TRUE)
```


```{r}
names(simp_dat)
```

Zap all the rows with any missing data (e.g., Reform didn't stand a candidate or no turnout data is available).

```{r}
for_analy_counts <- na.omit(simp_dat)
table1( ~ ., data = for_analy_counts |> dplyr::select(-post_label))
```


Create another dataframe with percentages, conditional on voting for one of these three parties.

```{r}
perc <- for_analy_counts |>
  pivot_longer(names_to = "Party",
               values_to = "Votes",
               cols = Ref:Con) |>
  group_by(post_label) |>
  mutate(Perc = 100 * Votes / sum(Votes))
perc
```

## Have a look

```{r dpi=300}
plot_votes <- perc |>
  ggplot(aes(turnout_percentage, Perc, colour = Party)) +
  geom_point(size = .7) +
  labs(
    x = "Turnout (%)",
    y = "Votes (%)",
    title = "UK general election 2024",
    caption = "Percentage votes are conditional on voting Tory, Labour, or Reform"
  ) +
  scale_color_manual(values = c("#0087dc", "#d50000", "#12b6cf"))
plot_votes +
  geom_smooth(se = FALSE,
              method = "loess",
              formula = y ~ x)
```



## Modelling

### Fit the models using the original counts

Now, onto the modelling. These use the generalized Wedderburn logit model.

```{r}
mod1_count <- colm(cbind(Con, Lab, Ref) ~ turnout_percentage, ref = 1, data = for_analy_counts)
mod2_count <- colm(cbind(Con, Lab, Ref) ~ turnout_percentage, ref = 2, data = for_analy_counts)
mod3_count <- colm(cbind(Con, Lab, Ref) ~ turnout_percentage, ref = 3, data = for_analy_counts)
```


```{r}
short_sum <- function(mod) {
  (mod |> summary())$Coefficients
}
```

```{r}
mod1_count |> short_sum()
mod2_count |> short_sum()
mod3_count |> short_sum()
```

The estimates/SE for turnout are over 14 for the comparisons of Labour versus Conservative and Reform versus conservative. They are around 4 for Labour versus Reform. So, all "statistically significant".

Now the coefficients. Choose the first model:


```{r}
bs <- mod1_count |> coef()
bs
```

These are as easy to interpret as logistic regression coefficients ;-) One trick is to exp them to get odds ratios.

```{r}
bs |> exp() |> round(2)
```

The intercepts give the odds of voting each party rather than Conservative when turnout is zero, which probably isn't useful... Each percentage point increase in turnout reduces the odds of voting Labour by 8% compared to Conservative.

It's easier to plot them:


```{r}
lab_vs_con_pred <- function(t)
  exp(bs["(Intercept)", "Lab"] + bs["turnout_percentage", "Lab"] * t)
ref_vs_con_pred <- function(t)
  exp(bs["(Intercept)", "Ref"] + bs["turnout_percentage", "Ref"] * t)
con_vs_con_pred <- function(t)
  exp(bs["(Intercept)", "Con"] + bs["turnout_percentage", "Con"] * t)

lab_pred <- function(turnout)
  100 * lab_vs_con_pred(turnout) /
  (lab_vs_con_pred(turnout) + con_vs_con_pred(turnout) + ref_vs_con_pred(turnout))
con_pred <- function(turnout)
  100 * con_vs_con_pred(turnout) /
  (lab_vs_con_pred(turnout) + con_vs_con_pred(turnout) + ref_vs_con_pred(turnout))
ref_pred <- function(turnout)
  100 * ref_vs_con_pred(turnout) /
  (lab_vs_con_pred(turnout) + con_vs_con_pred(turnout) + ref_vs_con_pred(turnout))
```

Note the Conservative predictions, since it is the base category:

```{r}
con_vs_con_pred(seq(0, 100, 10))
```

```{r dpi=300}
plot_votes +
  geom_function(
    fun = lab_pred,
    colour = "#d50000",
    size = 1,
    linetype = "dashed"
  ) +
  geom_function(
    fun = con_pred,
    colour = "#0087dc",
    size = 1,
    linetype = "dashed"
  ) +
  geom_function(
    fun = ref_pred,
    colour = "#12b6cf",
    size = 1,
    linetype = "dashed"
  ) +
  labs(title = "UK general election 2024 â€“ colm model predictions",
       caption = "Percentage votes are conditional on voting Tory, Labour, or Reform.\nDashed curves are colm model predictions; solid curves are loess.") +
  geom_smooth(se = FALSE,
              method = "loess",
              formula = y ~ x)
```

### Try again with percentages

If I've understood correctly, then using the percentages as outcomes, rather than counts, should give the same model coefficients:

```{r}
for_analy_percs <- perc |>
  select(-Votes) |>
  pivot_wider(names_from = "Party",
              values_from = "Perc")
for_analy_percs
```

```{r}
mod1_perc <- colm(cbind(Con, Lab, Ref) ~ turnout_percentage,
                  ref = 1,
                  data = for_analy_percs)
mod2_perc <- colm(cbind(Con, Lab, Ref) ~ turnout_percentage,
                  ref = 2,
                  data = for_analy_percs)
mod3_perc <- colm(cbind(Con, Lab, Ref) ~ turnout_percentage,
                  ref = 3,
                  data = for_analy_percs)
```

Reference level Con:

```{r}
mod1_count |> short_sum()
mod1_perc  |> short_sum()
```

Reference level Lab:

```{r}
mod2_count |> short_sum()
mod2_perc  |> short_sum()
```

Reference level Reform:

```{r}
mod3_count |> short_sum()
mod3_perc  |> short_sum()
```

And indeed they do.

The fitted values are different, though -- they are on the original scale:

```{r}
mod1_perc$fitted.values |> head()
```

```{r}
mod1_count$fitted.values |> head()
```


## Multiple imputation -- danger danger, going beyond the function manual...

Earlier we just zapped all missing data. Let's bring back missingness in the turnouts and only drop rows where one or more of the three parties didn't have a candidate.


```{r}
names(simp_dat)
```


```{r}
counts_with_miss <- simp_dat |>
  drop_na(Ref:Con)
```


```{r}
plot_pattern(counts_with_miss, rotate = TRUE)
```


Impute 50 times with default settings.

```{r}
set.seed(412121)
imp <- mice(counts_with_miss, m = 50, printFlag = FALSE)
```


Fit one of the models to each imputed dataset.

```{r}
mods <- with(imp, colm(cbind(Con, Lab, Ref) ~ turnout_percentage, ref = 1))
```


```{r}
get_params <- function(mod, comp = 1, pred) {
  the_coefs <- summary(mod)$Coefficients[[comp]]
  res <- the_coefs[pred, ]
  names(res) <- c("B", "SE")
  res
}
```

Quick check:

```{r}
get_params(mods$analyses[[1]], 1, "turnout_percentage")
```

```{r}
the_coefs <- map(mods$analyses, \(m) get_params(m, 1, "turnout_percentage")) |> bind_rows()
the_coefs$N <- nrow(counts_with_miss)
the_coefs
```


The estimate:

```{r}
pooled <- pool.scalar(the_coefs$B, the_coefs$SE^2, the_coefs$N)
res <- list(B = pooled$qbar, SE = sqrt(pooled$ubar))
res$t <- res$B / res$SE
res$df <- pooled$df[1]
res$p <- as.vector(pt(abs(res$t), df = res$df, lower.tail = FALSE) * 2)
res |> as_tibble()
```

## Comparison with multinomial regression

```{r}
library(nnet)
```


```{r}
mn_mod1 <- multinom(cbind(Con, Lab, Ref) ~ turnout_percentage,
                    Hess = TRUE, data = for_analy_counts)
mn_mod1 |> summary()
```

Compare with colm:

```{r}
mod1_count |> short_sum()
```

The estimates are similar. The multinomal regression SEs are much smaller, presumably because it can take advantage of the counts.



## What effect does having a Reform candidate have on the split between Tory/Labour?


They stood most places, so this is going to be tricky...

```{r}
reform_ass <- simp_dat |>
  drop_na(Lab, Con, turnout_percentage) |>
  mutate(reform_stood = 1 - is.na(Ref),
         Lab_perc = 100 * Lab / (Lab + Con))
```


```{r}
table1(~ Lab + Con + Ref + reform_stood + turnout_percentage, data = reform_ass)
```

```{r paged.print=FALSE}
reform_ass |>
  dplyr::filter(reform_stood == 0) |>
  arrange(-Lab / (Lab + Con)) |>
  select(-c(Ref, reform_stood))
```


```{r dpi=300}
reform_ass |>
  ggplot(aes(
    colour = factor(reform_stood, labels = c("No", "Yes")),
    x = turnout_percentage,
    y = Lab_perc
  )) +
  geom_point() +
  geom_smooth(method = "loess",
              formula = "y ~ x",
              se = TRUE) +
  labs(x = "Turnout (%)",
       y = "Percentage voting Lab (versus Con)",
       colour = "Reform stood",
       title = "Percentage voting Lab (vs. Con)") +
  ylim(0, NA)
```

Not enough points to have faith in this!


